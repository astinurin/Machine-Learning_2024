{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas 2: Decision Tree vs AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree Accuracy: 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1257\n",
      "           1       1.00      1.00      1.00      1181\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PJBS\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost Parameters: {'learning_rate': 1, 'n_estimators': 50}\n",
      "AdaBoost Accuracy: 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1257\n",
      "           1       1.00      1.00      1.00      1181\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Accuracy (Logistic Regression): 73.59%\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       151\n",
      "           1       0.63      0.59      0.61        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.71      0.70      0.70       231\n",
      "weighted avg       0.73      0.74      0.73       231\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Accuracy (SVM): 69.70%\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.88      0.79       151\n",
      "           1       0.61      0.35      0.44        80\n",
      "\n",
      "    accuracy                           0.70       231\n",
      "   macro avg       0.66      0.62      0.62       231\n",
      "weighted avg       0.68      0.70      0.67       231\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Accuracy (Decision Tree): 74.46%\n",
      "Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81       151\n",
      "           1       0.64      0.60      0.62        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.71      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n",
      "Accuracy (Ensemble Voting): 76.19%\n",
      "Classification Report (Ensemble Voting):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.83       151\n",
      "           1       0.69      0.57      0.63        80\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.72      0.73       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset mushrooms\n",
    "data_mushrooms = pd.read_csv('data/mushrooms.csv')\n",
    "\n",
    "# Ubah fitur kategori menjadi numerik \n",
    "label_encoder = LabelEncoder()\n",
    "for column in data_mushrooms.columns:\n",
    "    data_mushrooms[column] = label_encoder.fit_transform(data_mushrooms[column])\n",
    "\n",
    "# Split data menjadi fitur dan label\n",
    "X_mushrooms = data_mushrooms.drop('class', axis=1)\n",
    "y_mushrooms = data_mushrooms['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_mushrooms, X_test_mushrooms, y_train_mushrooms, y_test_mushrooms = train_test_split(X_mushrooms, y_mushrooms, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning untuk Decision Tree\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=dt, param_grid=param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_dt.fit(X_train_mushrooms, y_train_mushrooms)\n",
    "\n",
    "# Hasil terbaik dari Decision Tree\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test_mushrooms)\n",
    "\n",
    "# Akurasi dan laporan klasifikasi\n",
    "accuracy_dt = accuracy_score(y_test_mushrooms, y_pred_dt) * 100  # Akurasi dalam persen\n",
    "print(f\"Best Decision Tree Parameters: {grid_search_dt.best_params_}\")\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt:.2f}%\")\n",
    "print(classification_report(y_test_mushrooms, y_pred_dt))\n",
    "\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "# Hyperparameter tuning untuk AdaBoost\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_ada.fit(X_train_mushrooms, y_train_mushrooms)\n",
    "\n",
    "# Hasil terbaik dari AdaBoost\n",
    "best_ada = grid_search_ada.best_estimator_\n",
    "y_pred_ada = best_ada.predict(X_test_mushrooms)\n",
    "\n",
    "# Akurasi dan laporan klasifikasi\n",
    "accuracy_ada = accuracy_score(y_test_mushrooms, y_pred_ada) * 100  # Akurasi dalam persen\n",
    "print(f\"Best AdaBoost Parameters: {grid_search_ada.best_params_}\")\n",
    "print(f\"AdaBoost Accuracy: {accuracy_ada:.2f}%\")\n",
    "print(classification_report(y_test_mushrooms, y_pred_ada))\n",
    "\n",
    "# Load dataset diabetes\n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "# Cek data\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Ganti nilai nol yang tidak valid pada fitur\n",
    "feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "fill_values = SimpleImputer(missing_values=0, strategy=\"mean\", copy=False)\n",
    "df[feature_columns] = fill_values.fit_transform(df[feature_columns])\n",
    "\n",
    "# Split Data\n",
    "X = df[feature_columns]\n",
    "y = df.Outcome\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standarisasi Fitur\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Model Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning untuk Logistic Regression\n",
    "param_grid_logreg = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "grid_search_logreg = GridSearchCV(estimator=log_reg, param_grid=param_grid_logreg, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search_logreg.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediksi pada data test\n",
    "y_pred_logreg = grid_search_logreg.best_estimator_.predict(X_test_std)\n",
    "\n",
    "# Evaluasi Logistic Regression\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\"Accuracy (Logistic Regression): {accuracy_logreg * 100:.2f}%\")\n",
    "print(f\"Classification Report (Logistic Regression):\\n{classification_report(y_test, y_pred_logreg)}\")\n",
    "\n",
    "# Model SVM\n",
    "svc = SVC(kernel='poly', probability=True, random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning untuk SVM\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "grid_search_svc = GridSearchCV(estimator=svc, param_grid=param_grid_svc, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search_svc.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediksi pada data test\n",
    "y_pred_svc = grid_search_svc.best_estimator_.predict(X_test_std)\n",
    "\n",
    "# Evaluasi SVM\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(f\"Accuracy (SVM): {accuracy_svc * 100:.2f}%\")\n",
    "print(f\"Classification Report (SVM):\\n{classification_report(y_test, y_pred_svc)}\")\n",
    "\n",
    "# Model Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning untuk Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=dt, param_grid=param_grid_dt, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search_dt.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediksi pada data test\n",
    "y_pred_dt = grid_search_dt.best_estimator_.predict(X_test_std)\n",
    "\n",
    "# Evaluasi Decision Tree\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Accuracy (Decision Tree): {accuracy_dt * 100:.2f}%\")\n",
    "print(f\"Classification Report (Decision Tree):\\n{classification_report(y_test, y_pred_dt)}\")\n",
    "\n",
    "# Ensemble Voting\n",
    "log_reg_best = grid_search_logreg.best_estimator_  # Logistic Regression terbaik\n",
    "svc_best = grid_search_svc.best_estimator_         # SVM terbaik\n",
    "dt_best = grid_search_dt.best_estimator_           # Decision Tree terbaik\n",
    "\n",
    "# Ensemble Voting dengan soft voting\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_reg_best), ('svc', svc_best), ('dt', dt_best)], voting='soft')\n",
    "\n",
    "# Fit model pada data train\n",
    "voting_clf.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediksi pada data test\n",
    "y_pred_voting = voting_clf.predict(X_test_std)\n",
    "\n",
    "# Evaluasi Ensemble Voting\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f\"Accuracy (Ensemble Voting): {accuracy_voting * 100:.2f}%\")\n",
    "print(f\"Classification Report (Ensemble Voting):\\n{classification_report(y_test, y_pred_voting)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
